{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c11a5b",
   "metadata": {},
   "source": [
    "Unicode is a text encoding standard that maps characters to integer code points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b3c08",
   "metadata": {},
   "source": [
    "16进制 0-9表示0-9 A-B-C-D-E-F 表示10-15  0x开头"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5127c11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29275, '牛')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"牛\"),chr(29275)\n",
    "# ord() function to convert a single Unicode character\n",
    "# into its integer representation.\n",
    "# chr() function converts an integer Unicode code point \n",
    "# into a string with the corresponding character."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c25d4",
   "metadata": {},
   "source": [
    "### Problem (unicode1): Understanding Unicode (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347fc5e1",
   "metadata": {},
   "source": [
    "What Unicode character does chr(0) return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e922ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a\n",
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a1e4e1",
   "metadata": {},
   "source": [
    "How does this character’s string representation (__repr__()) differ from its printed representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c8ecfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'hello     \\\\n\\\\ta'\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(\"hello     \\n\\ta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb903366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello    \n",
      "\ta\n"
     ]
    }
   ],
   "source": [
    "print(\"hello    \\n\\ta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e69e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for character's string representation(repr()),\n",
    "# it will always add single quote around the input,\n",
    "# and will show escape character with single backslash(\\)\n",
    "\n",
    "# but for printed representation, \n",
    "# it will just the raw input, and will execute the escape character, \n",
    "# like \"\\n\",\"\\t\", it will do the tab/newline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c4a4c0",
   "metadata": {},
   "source": [
    "What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations:  \n",
    "\\>>> chr(0) \n",
    "\n",
    "\\>>> print(chr(0)) \n",
    "\n",
    "\\>>> \"this is a test\" + chr(0) + \"string\" \n",
    "\n",
    "\\>>> print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "625463da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83340a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d01b2e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) +\"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa2a520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) +\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1496cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because chr(0) represent the Unicode character 0, also call NULL character(NUL)\n",
    "# it is not a visiable character, will not show any content in screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd47cae",
   "metadata": {},
   "source": [
    "## 2.2 Unicode Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1a00275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello! こんにちは!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1776bd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!', 23)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf_8 = test_string.encode(\"utf-8\")\n",
    "utf_8,len(utf_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eacac7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(utf_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "852775e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([104,\n",
       "  101,\n",
       "  108,\n",
       "  108,\n",
       "  111,\n",
       "  33,\n",
       "  32,\n",
       "  227,\n",
       "  129,\n",
       "  147,\n",
       "  227,\n",
       "  130,\n",
       "  147,\n",
       "  227,\n",
       "  129,\n",
       "  171,\n",
       "  227,\n",
       "  129,\n",
       "  161,\n",
       "  227,\n",
       "  129,\n",
       "  175,\n",
       "  33],\n",
       " 23)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(utf_8),len(list(utf_8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16452f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello! こんにちは!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf_8.decode(\"utf-8\")\n",
    "# convert back from bytes to Unicodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42686a92",
   "metadata": {},
   "source": [
    "### Problem (unicode2): Unicode Encodings (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb752f29",
   "metadata": {},
   "source": [
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c1db53a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello! こんにちは!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d3a26ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!',\n",
       " 23,\n",
       " b'\\xff\\xfeh\\x00e\\x00l\\x00l\\x00o\\x00!\\x00 \\x00S0\\x930k0a0o0!\\x00',\n",
       " 28,\n",
       " b'\\xff\\xfe\\x00\\x00h\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00!\\x00\\x00\\x00 \\x00\\x00\\x00S0\\x00\\x00\\x930\\x00\\x00k0\\x00\\x00a0\\x00\\x00o0\\x00\\x00!\\x00\\x00\\x00',\n",
       " 56)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf_8 = test_string.encode(\"utf-8\")\n",
    "utf_16 = test_string.encode(\"utf-16\")\n",
    "utf_32 = test_string.encode(\"utf-32\")\n",
    "utf_8,len(utf_8),utf_16,len(utf_16),utf_32,len(utf_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fca715ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hello! こんにちは!', 'hello! こんにちは!', 'hello! こんにちは!')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode_8 = utf_8.decode(\"utf-8\")\n",
    "unicode_16 = utf_16.decode(\"utf-16\")\n",
    "unicode_32 = utf_32.decode(\"utf-32\")\n",
    "unicode_8,unicode_16,unicode_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd13d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bacause for utf-8, one english character only one byte,but for utf-16/32,\n",
    "# one character will take 2/4 bytes,which cost more memory\n",
    "# so we chose utf-8 for its most compatibility and storage efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45165661",
   "metadata": {},
   "source": [
    "(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32cbb55c",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m): \n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mbytes\u001b[39m([b])\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring]) \n\u001b[1;32m----> 3\u001b[0m \u001b[43mdecode_utf8_bytes_to_str_wrong\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m你好\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m, in \u001b[0;36mdecode_utf8_bytes_to_str_wrong\u001b[1;34m(bytestring)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m): \n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43m[\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbytestring\u001b[49m\u001b[43m]\u001b[49m)\n",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode_utf8_bytes_to_str_wrong\u001b[39m(bytestring: \u001b[38;5;28mbytes\u001b[39m): \n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m bytestring])\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe4 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes): \n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring]) \n",
    "decode_utf8_bytes_to_str_wrong(\"你好\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e41e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example, when the input is english character, one english character will\n",
    "# represented by one bytes, in this time, the func can work\n",
    "# but when the input is chinese,which one chinese chatacter is reprensented by \n",
    "# multi bytes,at this time , if we divide the bytes[111,112,113] one in one,\n",
    "# it will cause error.\n",
    "# UTF-8 is a \"variable length encoding\", not that each character only occupies one byte!\n",
    "# cannot decode a complete UTF-8 byte stream separately by single bytes, \n",
    "# but must give it to decode as a whole, and it will automatically \"group\" it into characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26983a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes): \n",
    "    return bytestring.decode(\"utf-8\")\n",
    "decode_utf8_bytes_to_str_wrong(\"你好\".encode(\"utf-8\"))\n",
    "# if we wanna fix the error,just decode the whole sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05cfc8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'hello', b'hello')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hello\"\n",
    "text.encode(\"utf-8\"),bytes(\"hello\".encode('utf-8'))\n",
    "# when input is already bytes, the bytes() will output without change input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9827d565",
   "metadata": {},
   "source": [
    "(c) Give a two byte sequence that does not decode to any Unicode character(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2c1819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x80': 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "b'\\xc2 ': 'utf-8' codec can't decode byte 0xc2 in position 0: invalid continuation byte\n",
      "b'\\xc0\\xaf': 'utf-8' codec can't decode byte 0xc0 in position 0: invalid start byte\n"
     ]
    }
   ],
   "source": [
    "for bad_bytes in [b'\\x80\\x80', b'\\xc2\\x20', b'\\xc0\\xaf']:\n",
    "    try:\n",
    "        print(bad_bytes.decode('utf-8'))\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"{bad_bytes}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69cf96fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bacause for utf-8,the head bytes should start by 0xxxxxxx(1 bytes)\n",
    "# for 2 bytes character,head: 110xxxxx. 3:1110xxxx 4: 11110xxx\n",
    "# no character start with \\x80 -> 1000000 will considered as continuation bytes\n",
    "# and the continuation bytes should start with 10xxxxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22edade6",
   "metadata": {},
   "source": [
    "## 2.3 Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295ba632",
   "metadata": {},
   "source": [
    "A compression algorithm that iteratively replaces (“merges”) the most frequent pair of bytes with a single, new unused index. Note that this algorithm adds subword tokens to our vocabulary to maximize the compression of our input sequences—if a word occurs in our input text enough times, it’ll be represented as a single subword unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f49018",
   "metadata": {},
   "source": [
    "“Vocabulary initialization” \n",
    "which is one-to-one mapping from bytestring token to interger ID,so there are 256 byte value, so the size of vocabulary is of size 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e50bc",
   "metadata": {},
   "source": [
    "“Pre-tokenization”,\n",
    "bacause directly count how often bytes occurs next to each other and then merging them starting with the most frequent pair of bytes will cost huge computation resources, and directly merging bytes across the corpus may result in token that differ only in punctuation(dog,dog!),so we use pre-tokenization,as coarse-grained tokenization over the corpus, such as 'text' appears 10 times,in this case, when we count how often the characters ‘t’ and ‘e’ appear next to each other, we will see that the word ‘text’ has ‘t’ and ‘e’ adjacent and we can increment their count by 10 instead of looking through the corpus.\n",
    "Since we’re training a byte-level BPE model, each pre-token is represented as a sequence of UTF-8 bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81da128a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "re.findall(PAT,\"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10202e2",
   "metadata": {},
   "source": [
    "When using it in your code, however, you should use re.finditer to avoid storing the pre-tokenized words as you construct your mapping from pre-tokens to their counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062de75b",
   "metadata": {},
   "source": [
    "\"Compute BPE merges\"\n",
    "Now that we’ve converted our input text into pre-tokens and represented each pre-token as a sequence of UTF-8 bytes\n",
    "\n",
    "At a high level, the BPE algorithm iteratively counts every pair of bytes and identifies the pair with the highest frequency (“A”, “B”). Every occurrence of this most frequent pair (“A”, “B”) is then merged, i.e., replaced with a new token “AB”. This new merged token is added to our vocabulary; as a result, the final vocabulary after BPE training is the size of the initial vocabulary (256 in our case), plus the number of BPE merge operations performed during training\n",
    "\n",
    "For efficiency during BPE training, we do not consider pairs that cross pre-token boundaries(some text, e and t are cross boundaries).2 When computing merges, deterministically break ties in pair frequency by preferring the lexicographically greater pair. For example, if the pairs (“A”, “B”), (“A”, “C”), (“B”, “ZZ”), and (“BA”, “A”) all have the highest frequency, we’d merge (“BA”, “A”),which has the biggest lexicographically (use max() func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c70bfa",
   "metadata": {},
   "source": [
    "\"Special tokens\"\n",
    "Often, some strings (e.g., <|endoftext|>) are used to encode metadata (e.g., boundaries between documents). When encoding text, it’s often desirable to treat some strings as “special tokens” that should never be split into multiple tokens (i.e., will always be preserved as a single token). For example, the end-of-sequence string <|endoftext|> should always be preserved as a single token (i.e., a single integer ID), so we know when to stop generating from the language model. These special tokens must be added to the vocabulary, so they have a corresponding fixed token ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77000837",
   "metadata": {},
   "source": [
    "Note that even a single byte is a bytes object in Python. There is no byte type in Python to represent a single byte, just as there is no char type in Python to represent a single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "418e8cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data/TinyStoriesV2-GPT4-train.txt\n",
      "Downloaded data/TinyStoriesV2-GPT4-valid.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "def download(url, filename):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "download(\n",
    "    'https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt',\n",
    "    'data/TinyStoriesV2-GPT4-train.txt'\n",
    ")\n",
    "download(\n",
    "    'https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt',\n",
    "    'data/TinyStoriesV2-GPT4-valid.txt'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4906cb",
   "metadata": {},
   "source": [
    "before we pretokenization,we should strip out all special tokens from our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8c9ac",
   "metadata": {},
   "source": [
    "BPE的训练过程：每次找到出现次数最多的相邻字节对（pair），然后把它们合并成一个新符号。\n",
    "朴素做法：每合并一次，就重新扫描整个语料，重新数所有的字节对频率。\n",
    "但是合并只影响与被合并对相关的地方，比如ab->x,受影响的地方只有原本包含ab的地方，xa，bx的新组合。其他地方没有变，不需要重新算。可以通过维护一个哈希表，存储每个字节对的计数，每次合并后，只更新相关 pair 的计数，而不是全量遍历语料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c23f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
